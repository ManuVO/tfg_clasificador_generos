   - Seccion `dataset`: rutas al nuevo CSV y estadisticas.
   - Seccion `audio`: parametros de espectrograma (`sample_rate`, `n_fft`, etc.).
   - Seccion `augmentation`: probabilidades e intensidades de transformaciones.
   - Seccion `model`: `num_classes` debe coincidir con el numero de generos.
   - Seccion `training`: hiperparametros, early stopping y scheduler.
3. Ejecute el entrenamiento con el archivo personalizado:
   ```powershell
   python src\train.py --config configs\mi_experimento.yaml
   ```
4. Si desea registrar datasets adicionales, agreguelos en `config.yaml` dentro de `datasets:` siguiendo el esquema de `gtzan` (propiedades `display_name` y `stages`).
5. Para inferencia, ajuste `configs/base/inference.yaml` o `configs/datasets/<dataset>/inference.yaml` indicando `checkpoint_path` y `class_mapping_path`.

## Comprobar el pipeline del clasificador
1. Con el entorno activo, ejecute la prueba sintentica:
   ```powershell
   python scripts\test_pipeline.py
   ```
2. El script genera un tono de prueba, aplica augmentaciones, calcula mel-espectrogramas y valida que las dimensiones coincidan con la configuracion resuelta.
3. Revise los logs en la terminal y la imagen `reports/tmp_pipeline/spec_example.png` para confirmar que la transformacion de audio a espectrograma funciona como se espera.

## Notas adicionales
- Los checkpoints y metricas se guardan siempre dentro de `experiments/`. El mejor checkpoint segun `val_loss` se almacena como `checkpoints\best_model.pt`.
- Ajuste `training.num_workers` si el hardware limita el numero de hilos de carga.
- Ante cambios de rutas o nuevos datasets, vuelva a ejecutar `src\data\preprocess.py` para recalcular `metadata.csv` y las estadisticas de normalizacion. -->

root@17cf3f2c7d0d:/workspace/tfg_clasificador_generos# nl -ba README.md | sed -n '1,220p'
     1  # ğŸ¶ Clasificador de gÃ©neros musicales
     2
     3  Â¡Bienvenido! Este repositorio recopila el Trabajo Fin de Grado dedicado a la **clasificaciÃ³n automÃ¡tica de gÃ©neros music
ales** a partir de audio. AquÃ­ encontrarÃ¡s todo lo necesario para preparar el dataset GTZAN, generar espectrogramas log-mel, ent
renar redes neuronales y sentar las bases de una aplicaciÃ³n que identifique el gÃ©nero musical de cualquier archivo que le propor
ciones.
     4
     5  ## ğŸ¯ Objetivo del proyecto
     6
     7  - ğŸ§ª **Meta a corto plazo (MVP)**: ofrecer una experiencia desde la terminal donde el usuario ejecute un comando, aporte
 un archivo de audio y reciba el gÃ©nero estimado por el modelo entrenado.
     8  - ğŸ§± **Estado actual**: el repositorio incluye scripts para preprocesar datos, entrenar y evaluar modelos, ademÃ¡s de uti
lidades para experimentar con distintas arquitecturas.
     9  - ğŸŒ  **VisiÃ³n a largo plazo**: evolucionar hacia una interfaz visual (escritorio y/o mÃ³vil) que permita cargar o grabar
audio y obtener el gÃ©nero musical de forma inmediata, convirtiendo el clasificador en una aplicaciÃ³n accesible para cualquier pe
rsona.
    10
    11  > ğŸš§ Este es un proyecto en construcciÃ³n. Cada secciÃ³n del README indica cÃ³mo puedes contribuir y quÃ© pasos seguir para
alcanzar la experiencia completa descrita arriba.
    12
    13  ## âœ¨ Â¿QuÃ© encontrarÃ¡s aquÃ­?
    14
    15  - ğŸšï¸ **Preprocesamiento de audio**: normalizaciÃ³n, resampleo y divisiÃ³n de pistas en fragmentos manejables.
    16  - ğŸ” **ExtracciÃ³n de caracterÃ­sticas**: cÃ¡lculo de espectrogramas log-mel y estadÃ­sticas reutilizables.
    17  - ğŸ§  **Modelos base**: CNN ligera (`CNNBaseline`) como punto de partida.
    18  - â™»ï¸ **Entrenamiento reproducible**: control de semillas, registros en TensorBoard y checkpoints.
    19  - ğŸ“Š **EvaluaciÃ³n automÃ¡tica**: reportes de mÃ©tricas, matriz de confusiÃ³n y anÃ¡lisis por gÃ©nero.
    20
    21  ## ğŸ§© Requisitos previos
    22
    23  - Python 3.10 o superior con `pip` y `venv` disponibles.
    24  - Dependencias de audio externas:
    25    - [FFmpeg](https://ffmpeg.org/) para que `pydub` procese formatos variados.
    26    - Bibliotecas de sistema como `libsndfile` o `portaudio`, segÃºn tu plataforma.
    27  - GPU con CUDA opcional pero recomendable para acelerar el entrenamiento (el cÃ³digo detecta CPU/GPU de forma automÃ¡tica)
.
    28
    29  ## ğŸš€ ConfiguraciÃ³n inicial
    30
    31  1. Clona el repositorio y sitÃºate en la carpeta del proyecto:
    32     ```bash
    33     git clone <url-del-repositorio>
    34     cd tfg_clasificador_generos
    35     ```
    36  2. **(Recomendado en Windows/PowerShell)** ejecuta el asistente automÃ¡tico:
    37     ```powershell
    38     ./init_env.ps1
    39     ```
    40     > â„¹ï¸ Este script comprueba la instalaciÃ³n de Python, crea (o reutiliza) el entorno virtual `.venv`, activa la sesiÃ³n,
actualiza `pip`, instala las dependencias de `requirements.txt`, configura `PYTHONPATH` y valida la importaciÃ³n de mÃ³dulos inter
nos. Al terminar, tendrÃ¡s todo listo para empezar a ejecutar los scripts del proyecto desde esa misma consola.
    41  3. **Si usas macOS o Linux**, realiza la preparaciÃ³n manual:
    42     ```bash
    43     python -m venv .venv
    44     source .venv/bin/activate
    45     pip install --upgrade pip
    46     pip install -r requirements.txt
    47     ```
    48     > ğŸ’¡ En cualquier sistema puedes instalar el paquete en modo editable para facilitar los imports: `pip install -e .`.
    49
    50  ## ğŸ¼ Preparar el dataset GTZAN
    51
    52  1. Descarga el dataset [GTZAN Genre Collection](http://marsyas.info/downloads/datasets.html) o una rÃ©plica fiable.
    53  2. Descomprime los audios de modo que queden en `data/raw/gtzan/genres_original/GENRE/*.wav` (una carpeta por gÃ©nero).
    54  3. Genera los segmentos, metadatos y estadÃ­sticas de normalizaciÃ³n:
    55     ```bash
    56     python src/data/preprocess.py --dataset gtzan --project-config config.yaml
    57     ```
    58     > ğŸ—‚ï¸ Los resultados aparecerÃ¡n en `data/processed/gtzan/` y se reutilizan durante el entrenamiento.
    59
    60  ## ğŸ‹ï¸â€â™€ï¸ Entrenar un modelo
    61
    62  1. Revisa la configuraciÃ³n en `configs/datasets/gtzan/training.yaml` (Ã©pocas, batch size, scheduler, augmentationsâ€¦).
    63  2. Lanza el entrenamiento:
    64     ```bash
    65     python src/train.py --dataset gtzan --project-config config.yaml
    66     ```
    67  3. EncontrarÃ¡s los artefactos en `experiments/<nombre_experimento>/run_<timestamp>/`:
    68     - `checkpoints/best_model.pt`
    69     - `logs/` para TensorBoard (`tensorboard --logdir experiments`)
    70     - `metrics.csv` y la configuraciÃ³n resuelta (`config.yaml`)
    71
    72  ## âœ… Evaluar un checkpoint
    73
    74  ```bash
    75  python src/eval.py --dataset gtzan --project-config config.yaml \
    76      --run_dir experiments/<nombre_experimento>/run_<timestamp>/
    77  ```
    78
    79  El comando genera `classification_report.txt` y `confusion_matrix.png` en el directorio de la ejecuciÃ³n (o en el indicad
o con la opciÃ³n `--out_dir`).
    80
    81  ## ğŸ§­ Mapa del repositorio
    82
    83  ```text
    84  ğŸ“ tfg_clasificador_generos
    85  â”œâ”€â”€ ğŸ“‚ configs/             â†’ Plantillas de configuraciÃ³n para datasets y stages
    86  â”œâ”€â”€ ğŸ“‚ data/                â†’ Datos brutos (`raw`) y procesados (`processed`) â€” se aÃ±aden manualmente
    87  â”œâ”€â”€ ğŸ“‚ reports/             â†’ Informes, grÃ¡ficos y resultados exportados
    88  â”œâ”€â”€ ğŸ“‚ scripts/             â†’ Utilidades y experimentos rÃ¡pidos (ej. notebooks, smoke tests)
    89  â”œâ”€â”€ ğŸ“‚ src/
    90  â”‚   â”œâ”€â”€ ğŸ“‚ data/            â†’ Preprocesado y definiciÃ³n de datasets
    91  â”‚   â”œâ”€â”€ ğŸ“‚ features/        â†’ ExtracciÃ³n de caracterÃ­sticas y augmentations
    92  â”‚   â”œâ”€â”€ ğŸ“‚ models/          â†’ Arquitecturas de redes neuronales
    93  â”‚   â”œâ”€â”€ ğŸ§¾ train.py         â†’ Punto de entrada para entrenar
    94  â”‚   â””â”€â”€ ğŸ§¾ eval.py          â†’ EvaluaciÃ³n de checkpoints entrenados
    95  â”œâ”€â”€ ğŸ§¾ config.yaml          â†’ ConfiguraciÃ³n maestra del proyecto
    96  â”œâ”€â”€ ğŸ§¾ requirements.txt     â†’ Dependencias de Python
    97  â”œâ”€â”€ ğŸ§¾ init_env.ps1         â†’ Asistente para preparar el entorno en PowerShell
    98  â””â”€â”€ ğŸ§¾ README.md            â†’ Esta guÃ­a
    99  ```
   100
   101  ## ğŸ›¤ï¸ Hoja de ruta hacia la experiencia completa
   102
   103  1. ğŸ–¥ï¸ **CLI de inferencia**: exponer un comando `python src/predict.py --audio <ruta>` que utilice el mejor checkpoint di
sponible y devuelva el gÃ©nero estimado.
   104  2. ğŸ§ª **ValidaciÃ³n con audios reales**: recopilar ejemplos propios, ejecutar el pipeline de inferencia y documentar acie
rtos/errores en `reports/`.
   105  3. ğŸ¤ **Feedback de usuarios**: diseÃ±ar pruebas con amigos o compaÃ±eros para mejorar usabilidad y mensajes de la herrami
enta en terminal.
   106  4. ğŸ“± **VisiÃ³n futura (UI)**: prototipar una interfaz grÃ¡fica (desktop/mÃ³vil) que permita seleccionar o grabar un audio
y ver el gÃ©nero resultante al instante.
   107
   108  Con esta guÃ­a visual tendrÃ¡s claro el propÃ³sito del proyecto, sabrÃ¡s en quÃ© punto nos encontramos y podrÃ¡s contribuir a
que el clasificador evolucione desde la terminal hasta convertirse en una aplicaciÃ³n accesible para todos.
