<!-- # tfg_clasificador_generos (WIP)

## Descripcion general
El proyecto implementa un clasificador de genero musical basado en espectrogramas mel y redes convolucionales. Incluye todo el pipeline: descarga y segmentacion del dataset GTZAN, extraccion de caracteristicas, augmentaciones, entrenamiento, evaluacion y utilidades para comprobar la integridad del flujo. El objetivo es ofrecer una base reproducible para experimentos de aprendizaje profundo orientados a audio.

## Caracteristicas principales
- Pipeline reproducible configurable mediante archivos YAML por etapa (training, evaluation, inference).
- Preprocesamiento automatizado de GTZAN (descarga, segmentacion, estadisticas de normalizacion y etiquetas estratificadas).
- Modelo `CNNBaseline` con augmentaciones de waveform y spectrograma, early stopping, scheduler y tracking con TensorBoard.
- Scripts dedicados para entrenamiento (`src/train.py`), evaluacion (`src/eval.py`) y verificacion rapida del pipeline (`scripts/test_pipeline.py`).
- Reportes de salida completos: checkpoints, metricas CSV/JSON, matriz de confusion e informe de clasificacion.

## Requisitos previos
- Windows 10/11, macOS o Linux con acceso a PowerShell o bash.
- Python 3.10 (recomendado) con `pip` y soporte para `venv`.
- Git (solo si se clona el repositorio por primera vez).
- FFmpeg en PATH si se desea soporte extendido de decodificacion (requerido por `pydub` en casos excepcionales).
- (Opcional) GPU con CUDA para acelerar el entrenamiento.
- (Opcional) Kaggle CLI configurada con credenciales (`~/.kaggle/kaggle.json`) para descargar GTZAN automaticamente.

## Mapa del repositorio
```
tfg_clasificador_generos/
|-- configs/               # Plantillas y overrides por dataset/etapa
|   |-- base/
|   |   |-- training.yaml
|   |   `-- inference.yaml
|   |-- datasets/
|   |   `-- gtzan/
|   |       |-- training.yaml
|   |       `-- inference.yaml
|   `-- template.yaml       # Punto de partida para nuevas configuraciones
|-- data/                   # Directorios raw/processed (se crean al preparar datasets)
|-- experiments/            # Runs de entrenamiento con checkpoints y metricas
|-- reports/                # Salidas auxiliares (figuras, tablas comparativas, pruebas de pipeline)
|-- scripts/
|   `-- test_pipeline.py    # Verificacion rapida del pipeline de audio
|-- src/
|   |-- configuration.py    # Resolucion jerarquica de configuraciones
|   |-- train.py            # Entrenamiento end-to-end
|   |-- eval.py             # Evaluacion de checkpoints
|   `-- data/, features/, models/  # Modulos internos
|-- init_env.ps1            # Inicializacion/activacion del entorno virtual
|-- config.yaml             # Configuracion maestra del proyecto
|-- requirements.txt        # Dependencias base
`-- pyproject.toml          # Metadata del paquete y dependencias opcionales
```

## Preparacion del entorno
1. Abra una terminal en la raiz del proyecto.
2. Verifique que Python 3.10 este disponible ejecutando:
   ```powershell
   python --version
   ```
   Si no aparece, instale Python 3.10 y reinicie la terminal.
3. Cree y active el entorno virtual junto con las dependencias usando el script incluido:
   ```powershell
   .\init_env.ps1
   ```
   El script crea `.venv`, activa la shell dentro del entorno, instala `requirements.txt` y define `PYTHONPATH=src`.
4. (Opcional) Para futuras sesiones active manualmente el entorno sin reinstalar dependencias:
   ```powershell
   .\.venv\Scripts\Activate.ps1
   ```

## Procesar datasets
1. Mantenga el entorno virtual activo (`(.venv)` en el prompt).
2. Descargue GTZAN. Con la CLI de Kaggle configurada:
   ```powershell
   python src\data\download_gtzan.py
   ```
   El script guarda el contenido en `data/raw/gtzan`.
   Si prefiere hacerlo manualmente, descargue el ZIP desde Kaggle, descomprimalo y mueva la carpeta de generos a `data/raw/gtzan` siguiendo la estructura `Data/genres_original`.
3. Genere los segmentos y el CSV maestro ejecutando el preprocesado:
   ```powershell
   python src\data\preprocess.py
   ```
   Este paso crea:
   - Audio segmentado en `data/processed/gtzan/segments/<split>/<genero>/`.
   - El archivo `data/processed/gtzan/metadata.csv` con rutas y metadatos.
   - Estadisticas de normalizacion en `data/processed/gtzan/norm_stats.json`.
4. Revise que las rutas generadas coincidan con las declaradas en `configs/datasets/gtzan/training.yaml`. Si cambia ubicaciones, ajuste `csv_path` y `norm_stats_path` en la configuracion.

## Entrenar el modelo
1. Active el entorno (`.venv`) si no lo esta.
2. Lance el entrenamiento por defecto (usa el dataset y configuracion definidos en `config.yaml`):
   ```powershell
   python src\train.py
   ```
   El script crea una carpeta `experiments/<experiment_name>/run_YYYYMMDD_HHMMSS/` con checkpoints, metricas y reportes.
3. Para especificar otro dataset o un YAML completo resolviendo overrides personalizados:
   ```powershell
   python src\train.py --dataset gtzan
   python src\train.py --config configs\template.yaml
   ```
   Use `--project-config` si quiere apuntar a un `config.yaml` alternativo.
4. Supervise el progreso con TensorBoard apuntando a `experiments/<...>/logs`:
   ```powershell
   tensorboard --logdir experiments
   ```
   (Instale TensorBoard si no esta incluido en los requisitos.)

## Evaluar un checkpoint
1. Identifique la carpeta del run (por ejemplo `experiments/gtzan_cnn/run_20250923_123850`).
2. Ejecute la evaluacion proporcionando el directorio del run y, opcionalmente, un checkpoint especifico:
   ```powershell
   python src\eval.py --run_dir experiments\gtzan_cnn\run_20250923_123850
   ```
   Para usar otro checkpoint o salida:
   ```powershell
   python src\eval.py --run_dir <run_dir> --ckpt <ruta_al_pt> --out_dir reports\eval_custom
   ```
3. Revise `classification_report.txt`, `confusion_matrix.png` y las metricas impresas en la terminal. Los valores se calculan sobre el split `test` indicado en el CSV.

## Personalizar la configuracion
1. `config.yaml` define:
   - `defaults.dataset` y `defaults.stage` (dataset y etapa por defecto).
   - La ruta base de cada etapa (`configs/base/...`).
   - Los overrides por dataset (`configs/datasets/<dataset>/<stage>.yaml`).
2. Para crear una configuracion nueva copie `configs/template.yaml` a `configs/mi_experimento.yaml` y adapte:
   - Seccion `dataset`: rutas al nuevo CSV y estadisticas.
   - Seccion `audio`: parametros de espectrograma (`sample_rate`, `n_fft`, etc.).
   - Seccion `augmentation`: probabilidades e intensidades de transformaciones.
   - Seccion `model`: `num_classes` debe coincidir con el numero de generos.
   - Seccion `training`: hiperparametros, early stopping y scheduler.
3. Ejecute el entrenamiento con el archivo personalizado:
   ```powershell
   python src\train.py --config configs\mi_experimento.yaml
   ```
4. Si desea registrar datasets adicionales, agreguelos en `config.yaml` dentro de `datasets:` siguiendo el esquema de `gtzan` (propiedades `display_name` y `stages`).
5. Para inferencia, ajuste `configs/base/inference.yaml` o `configs/datasets/<dataset>/inference.yaml` indicando `checkpoint_path` y `class_mapping_path`.

## Comprobar el pipeline del clasificador
1. Con el entorno activo, ejecute la prueba sintentica:
   ```powershell
   python scripts\test_pipeline.py
   ```
2. El script genera un tono de prueba, aplica augmentaciones, calcula mel-espectrogramas y valida que las dimensiones coincidan con la configuracion resuelta.
3. Revise los logs en la terminal y la imagen `reports/tmp_pipeline/spec_example.png` para confirmar que la transformacion de audio a espectrograma funciona como se espera.

## Notas adicionales
- Los checkpoints y metricas se guardan siempre dentro de `experiments/`. El mejor checkpoint segun `val_loss` se almacena como `checkpoints\best_model.pt`.
- Ajuste `training.num_workers` si el hardware limita el numero de hilos de carga.
- Ante cambios de rutas o nuevos datasets, vuelva a ejecutar `src\data\preprocess.py` para recalcular `metadata.csv` y las estadisticas de normalizacion. -->

root@17cf3f2c7d0d:/workspace/tfg_clasificador_generos# nl -ba README.md | sed -n '1,220p'
     1  # üé∂ Clasificador de g√©neros musicales
     2
     3  ¬°Bienvenido! Este repositorio recopila el Trabajo Fin de Grado dedicado a la **clasificaci√≥n autom√°tica de g√©neros music
ales** a partir de audio. Aqu√≠ encontrar√°s todo lo necesario para preparar el dataset GTZAN, generar espectrogramas log-mel, ent
renar redes neuronales y sentar las bases de una aplicaci√≥n que identifique el g√©nero musical de cualquier archivo que le propor
ciones.
     4
     5  ## üéØ Objetivo del proyecto
     6
     7  - üß™ **Meta a corto plazo (MVP)**: ofrecer una experiencia desde la terminal donde el usuario ejecute un comando, aporte
 un archivo de audio y reciba el g√©nero estimado por el modelo entrenado.
     8  - üß± **Estado actual**: el repositorio incluye scripts para preprocesar datos, entrenar y evaluar modelos, adem√°s de uti
lidades para experimentar con distintas arquitecturas.
     9  - üå† **Visi√≥n a largo plazo**: evolucionar hacia una interfaz visual (escritorio y/o m√≥vil) que permita cargar o grabar
audio y obtener el g√©nero musical de forma inmediata, convirtiendo el clasificador en una aplicaci√≥n accesible para cualquier pe
rsona.
    10
    11  > üöß Este es un proyecto en construcci√≥n. Cada secci√≥n del README indica c√≥mo puedes contribuir y qu√© pasos seguir para
alcanzar la experiencia completa descrita arriba.
    12
    13  ## ‚ú® ¬øQu√© encontrar√°s aqu√≠?
    14
    15  - üéöÔ∏è **Preprocesamiento de audio**: normalizaci√≥n, resampleo y divisi√≥n de pistas en fragmentos manejables.
    16  - üîç **Extracci√≥n de caracter√≠sticas**: c√°lculo de espectrogramas log-mel y estad√≠sticas reutilizables.
    17  - üß† **Modelos base**: CNN ligera (`CNNBaseline`) como punto de partida.
    18  - ‚ôªÔ∏è **Entrenamiento reproducible**: control de semillas, registros en TensorBoard y checkpoints.
    19  - üìä **Evaluaci√≥n autom√°tica**: reportes de m√©tricas, matriz de confusi√≥n y an√°lisis por g√©nero.
    20
    21  ## üß© Requisitos previos
    22
    23  - Python 3.10 o superior con `pip` y `venv` disponibles.
    24  - Dependencias de audio externas:
    25    - [FFmpeg](https://ffmpeg.org/) para que `pydub` procese formatos variados.
    26    - Bibliotecas de sistema como `libsndfile` o `portaudio`, seg√∫n tu plataforma.
    27  - GPU con CUDA opcional pero recomendable para acelerar el entrenamiento (el c√≥digo detecta CPU/GPU de forma autom√°tica)
.
    28
    29  ## üöÄ Configuraci√≥n inicial
    30
    31  1. Clona el repositorio y sit√∫ate en la carpeta del proyecto:
    32     ```bash
    33     git clone <url-del-repositorio>
    34     cd tfg_clasificador_generos
    35     ```
    36  2. **(Recomendado en Windows/PowerShell)** ejecuta el asistente autom√°tico:
    37     ```powershell
    38     ./init_env.ps1
    39     ```
    40     > ‚ÑπÔ∏è Este script comprueba la instalaci√≥n de Python, crea (o reutiliza) el entorno virtual `.venv`, activa la sesi√≥n,
actualiza `pip`, instala las dependencias de `requirements.txt`, configura `PYTHONPATH` y valida la importaci√≥n de m√≥dulos inter
nos. Al terminar, tendr√°s todo listo para empezar a ejecutar los scripts del proyecto desde esa misma consola.
    41  3. **Si usas macOS o Linux**, realiza la preparaci√≥n manual:
    42     ```bash
    43     python -m venv .venv
