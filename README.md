# ğŸµ Clasificador de gÃ©neros musicales (WIP)

Este repositorio forma parte de un **Trabajo Fin de Grado (TFG)** dedicado a la **clasificaciÃ³n automÃ¡tica de gÃ©neros musicales** a partir de audio.  
El proyecto implementa un pipeline completo: desde la **preparaciÃ³n de un dataset**, la **generaciÃ³n de espectrogramas log-mel**, el **entrenamiento de redes neuronales convolucionales (CNNs)** y su **evaluaciÃ³n**, hasta sentar las bases de una futura aplicaciÃ³n capaz de predecir el gÃ©nero musical de cualquier archivo de audio.

---

## ğŸ”„ Flujo del pipeline

### VersiÃ³n grÃ¡fica
*(Ubicada en `docs/pipeline.png`, se puede generar con Draw.io, Mermaid u otra herramienta.)*

### VersiÃ³n textual
```text
ğŸ¼ Dataset
       â”‚
       â–¼
ğŸšï¸ Preprocesamiento + extracciÃ³n de caracterÃ­sticas
       â”‚
       â–¼
ğŸ§  Entrenamiento de la red neuronal
       â”‚
       â–¼
ğŸ“¦ Modelo entrenado
       â”‚
       â–¼
ğŸµ Nuevo archivo de audio
       â”‚
       â–¼
âœ… ClasificaciÃ³n â†’ GÃ©nero musical
```

---

## ğŸ¯ Objetivos del proyecto
- ğŸš€ **Meta a corto plazo (MVP):** ofrecer una experiencia reproducible desde terminal: el usuario prepara un dataset, entrena un modelo y obtiene mÃ©tricas y reportes.  
- ğŸ“¦ **Estado actual:** scripts para preprocesar datos, entrenar y evaluar modelos, ademÃ¡s de utilidades para comprobar la integridad del pipeline.  
- ğŸŒˆ **VisiÃ³n a largo plazo:** evolucionar hacia una interfaz (desktop o mÃ³vil) que permita cargar o grabar audio y obtener el gÃ©nero musical en tiempo real.  
- ğŸ”® **Posibles extensiones:** integrar datasets adicionales como FMA, MSD+Last.fm, MagnaTagATune, MTG-Jamendo, Homburg, Ballroom, ISMIR 2004, GiantSteps, AudioSet o Spotify MPD.  

---

## ğŸ—ºï¸ Mapa del repositorio
```text
ğŸ“ tfg_clasificador_generos
â”œâ”€â”€ ğŸ“‚ configs/             â†’ Plantillas de configuraciÃ³n (base, datasets, overrides)
â”œâ”€â”€ ğŸ“‚ data/                â†’ Datos brutos (`raw`) y procesados (`processed`)
â”œâ”€â”€ ğŸ“‚ experiments/         â†’ Resultados de entrenamiento (checkpoints, mÃ©tricas, logs)
â”œâ”€â”€ ğŸ“‚ reports/             â†’ Salidas auxiliares (figuras, informes, comparativas)
â”œâ”€â”€ ğŸ“‚ scripts/             â†’ Utilidades rÃ¡pidas (ej. `test_pipeline.py`)
â”œâ”€â”€ ğŸ“‚ src/
â”‚   â”œâ”€â”€ ğŸ“‚ data/            â†’ Descarga y preprocesamiento de datasets
â”‚   â”œâ”€â”€ ğŸ“‚ features/        â†’ ExtracciÃ³n de espectrogramas y augmentations
â”‚   â”œâ”€â”€ ğŸ“‚ models/          â†’ DefiniciÃ³n de arquitecturas (ej. `CNNBaseline`)
â”‚   â”œâ”€â”€ ğŸ§¾ train.py         â†’ Entrenamiento end-to-end
â”‚   â””â”€â”€ ğŸ§¾ eval.py          â†’ EvaluaciÃ³n de checkpoints
â”œâ”€â”€ ğŸ§¾ config.yaml          â†’ ConfiguraciÃ³n maestra del proyecto
â”œâ”€â”€ ğŸ§¾ requirements.txt     â†’ Dependencias base
â”œâ”€â”€ ğŸ§¾ init_env.ps1         â†’ Script para entorno virtual en PowerShell
â””â”€â”€ ğŸ§¾ README.md            â†’ Esta guÃ­a
```

---

## ğŸ§° Â¿QuÃ© encontrarÃ¡s aquÃ­?
- ğŸšï¸ **Preprocesamiento reproducible:** descarga, segmentaciÃ³n y normalizaciÃ³n de datasets.  
- ğŸ“Š **ExtracciÃ³n de caracterÃ­sticas:** espectrogramas log-mel, estadÃ­sticas y augmentations.  
- ğŸ§  **Modelos base:** CNN ligera con early stopping y scheduler.  
- ğŸ“ˆ **Entrenamiento controlado:** configuraciones YAML, semillas fijas y logs en TensorBoard.  
- âœ… **EvaluaciÃ³n automÃ¡tica:** mÃ©tricas CSV/JSON, clasificaciÃ³n por gÃ©nero y matriz de confusiÃ³n.  
- ğŸ§ª **VerificaciÃ³n rÃ¡pida del pipeline:** script sintÃ©tico para comprobar la correcta conversiÃ³n audioâ†’espectrograma.  

---

## ğŸ§± Requisitos previos
- ğŸ Python 3.10 (recomendado) con `pip` y `venv`.  
- ğŸ§ FFmpeg en el `PATH`.  
- ğŸ’» Sistemas soportados: Windows 10/11, macOS, Linux.  
- âš¡ Opcional: GPU con CUDA para acelerar el entrenamiento.  
- ğŸ“¥ Opcional: Kaggle CLI configurada para descargar GTZAN automÃ¡ticamente.  

---

## âš™ï¸ ConfiguraciÃ³n inicial
### Windows / PowerShell
```powershell
./init_env.ps1
```
Crea `.venv`, instala dependencias, configura `PYTHONPATH` y activa el entorno.

### macOS / Linux
```bash
python -m venv .venv
source .venv/bin/activate
pip install --upgrade pip
pip install -r requirements.txt
```

---

## ğŸ§ Preparar el dataset GTZAN
1. Descarga el dataset GTZAN desde una fuente fiable.  
2. ColÃ³calo dentro de `data/raw/`, quedando asÃ­:  
   ```text
   data/raw/gtzan/genres_original/<GENRE>/*.wav
   ```
   *(El dataset ya viene estructurado en carpetas de gÃ©neros; basta con ubicarlo en `data/raw/`.)*  
3. Ejecuta el preprocesado:  
   ```bash
   python src/data/preprocess.py --dataset gtzan --project-config config.yaml
   ```
   Esto genera:  
   - `data/processed/gtzan/segments/`  
   - `data/processed/gtzan/metadata.csv`  
   - `data/processed/gtzan/norm_stats.json`  

---

## ğŸ‹ï¸ Entrenar un modelo
```bash
python src/train.py --dataset gtzan --project-config config.yaml
```
Resultados en `experiments/<nombre_experimento>/run_<timestamp>/`:  
- `checkpoints/best_model.pt`  
- `metrics.csv`, `classification_report.txt`  
- Logs para TensorBoard (`tensorboard --logdir experiments`)  

---

## ğŸ§ª Evaluar un checkpoint
```bash
python src/eval.py --run_dir experiments/<experimento>/run_<timestamp>/
```
Genera reportes (`classification_report.txt`, `confusion_matrix.png`) en el directorio del run o en `--out_dir` personalizado.

---

## âš¡ Personalizar la configuraciÃ³n
Configura datasets, parÃ¡metros de audio, augmentations, arquitectura y entrenamiento editando los YAML de `configs/`.

Ejemplo:
```bash
python src/train.py --config configs/mi_experimento.yaml
```

### Ajustes recomendados para `gtzan`
- **Augmentations en forma de onda (`augmentation.waveform`)** â†’ se han rebajado las probabilidades a valores entre 0.3 y 0.4 para que cada clip tenga mÃ¡s frecuencia sin distorsiones extremas. AsÃ­ la red aprende patrones limpios y, al mismo tiempo, mantiene algo de diversidad para generalizar.
- **SpecAugment (`augmentation.spec_augment`)** â†’ ahora aplica menos mÃ¡scaras (2 de frecuencia, 3 temporales) con un ancho moderado. Esto oculta partes del espectrograma para evitar sobreajuste, pero deja suficiente informaciÃ³n para que la validaciÃ³n no se dispare.
- **Ã‰pocas y tamaÃ±o de lote** â†’ `training.epochs` sube a 40 y `batch_size` se mantiene en 32. Con lotes pequeÃ±os el optimizador ve mÃ¡s iteraciones por Ã©poca y la paciencia extra permite que el aprendizaje estabilice antes de que el early stopping corte.
- **Tasa de aprendizaje y weight decay** â†’ `learning_rate` se fija en `1e-3` para arrancar mÃ¡s fuerte y `weight_decay` se reduce a `1e-4`. Este equilibrio acelera la caÃ­da de la pÃ©rdida de entrenamiento sin forzar demasiada penalizaciÃ³n L2 que frene la capacidad del modelo.
- **Scheduler** â†’ `ReduceLROnPlateau` conserva un factor moderado (`0.5`) y amplÃ­a la `patience` a 3 con `threshold=0.005`. Solo baja el *LR* cuando la validaciÃ³n deja de mejorar de manera significativa, evitando reducciones prematuras.
- **Early stopping** â†’ `patience=10` y `min_delta=0.005` filtran mejoras marginales. El entrenamiento solo se detendrÃ¡ cuando la validaciÃ³n estÃ© estancada varios ciclos consecutivos.
- **Semilla (`seed`)** â†’ sigue en 42 para reproducibilidad. Cambiarla te permite comprobar la robustez del entrenamiento.
- **Barra de progreso (`progress_bar`)** â†’ permanece desactivada en la base y se activa Ãºnicamente en `configs/datasets/gtzan/training.yaml`. Puedes omitir esta clave en datasets donde no quieras barra; el cÃ³digo asume `false` si no existe.

---

## ğŸ” Comprobar el pipeline
```bash
python scripts/test_pipeline.py
```
Verifica con un audio sintÃ©tico que la transformaciÃ³n a espectrograma funciona correctamente.  

---

## ğŸ“ Notas adicionales
- Los checkpoints se guardan en `experiments/`; el mejor modelo se guarda como `checkpoints/best_model.pt`.
- Ajusta `training.num_workers` si tu hardware limita hilos.
- Controla la regularizaciÃ³n L2 del optimizador con `training.weight_decay` (por defecto `1e-4`; fija `0.0` para desactivarla).
- Cambios en rutas/datasets â†’ ejecutar de nuevo el preprocesado.

---

## ğŸ—“ï¸ Hoja de ruta
1. **CLI de inferencia:** comando `python src/predict.py --audio <ruta>` que devuelva el gÃ©nero estimado.  
2. **ValidaciÃ³n externa:** probar con audios reales y documentar resultados.  
3. **Feedback de usuarios:** mejorar usabilidad y mensajes en terminal.  
4. **ExtensiÃ³n a nuevos datasets:** explorar FMA, MSD+Last.fm, MagnaTagATune, MTG-Jamendo, Homburg, Ballroom, ISMIR 2004, GiantSteps, AudioSet, Spotify MPD.  
5. **Interfaz futura:** prototipos visuales (desktop/mÃ³vil).  

---

ğŸ“Œ **Estado:** proyecto en construcciÃ³n (WIP).  
Cada contribuciÃ³n acerca este clasificador a su visiÃ³n final: una herramienta reproducible, accesible y acadÃ©micamente sÃ³lida para la clasificaciÃ³n de gÃ©neros musicales.  
